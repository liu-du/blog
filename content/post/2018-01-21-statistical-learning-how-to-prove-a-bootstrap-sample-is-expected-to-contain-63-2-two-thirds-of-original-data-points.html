---
title: 'Statistical Learning: a bootstrap sample contains
  two thirds of original data points?'
author: ''
date: '2018-01-21'
slug: statistical-learning-how-to-prove-a-bootstrap-sample-is-expected-to-contain-63-2-two-thirds-of-original-data-points
categories:
  - Statistics
tags:
  - Statistics
  - Statistical Learning
  - Bootstrap
---



<div id="intro" class="section level2">
<h2>Intro</h2>
<p>Recently, I am studying <a href="https://web.stanford.edu/~hastie/Papers/ESLII.pdf">The Elements of Statistical Learning</a> out of my own interest. An interesting topic they covered briefly on their online <a href="https://youtu.be/BzHz0J9a6k0">videos</a> is bootstrap. Bootstrap, in the simplest terms, is the process of generating samples from sample. Say you collected a sample of n data points, a bootstrap sample is generated by sampling n data points from the original sample you collected <strong>with replacement</strong>. (So Given a sample size of n, you are able to generate <span class="math inline">\(n^n\)</span> bootstrap samples.)</p>
<p>One of the assertion that they made on bootstrap is that bootstrap samples contains, on average, around two thirds of the original data points (Or to be more precise, 63.2%). And in the online video Rob said it is not hard to prove it.</p>
<p>I believed in him and tried to prove it myself, only to find myself still pondering after 2 hours. It turns out that there are (at least) two ways to prove it: an easy way and a hard way. Unfortunately and unintentionally, I realized that I have chosen the hard way to prove it… (An excuse why I couldn’t prove such an “easy” problem).</p>
<p>As an afterthought, I realised I was trying to characterize the entire distribution of the <em>Number of unique data points in a bootstrap sample</em> while I could have simply computed the mean of it (more excuse for myself).</p>
</div>
<div id="easy-way-first" class="section level2">
<h2>Easy way first</h2>
<p>Let <span class="math inline">\(Y\)</span> be the number of unique data points in a bootstrap sample:</p>
<p><span class="math display">\[Y = \sum_{i = 1}^{n} {I(x_i \in S)}\]</span></p>
<p>where <span class="math inline">\(S\)</span> is the original sample, <span class="math inline">\(x_i\)</span> is the <span class="math inline">\(i^{th}\)</span> data point in the <strong>original sample</strong> and n is number of data points in the bootstrap sample (which is also the number of data points in the original sample). Therefore, the expected number of unique data points in a bootstrap sample is:</p>
<p><span class="math display">\[
\begin{align}
\mathbb{E} [Y] 
&amp; = \mathbb{E} \left[\sum_{i = 1}^{n} {I(x_i \in S)}\right] \\
&amp; = \sum_{i = 1}^{n} \mathbb{E} \left[ {I(x_i \in S)}\right] \\
&amp; = \sum_{i = 1}^{n} \mathbb{P} \left( x_i \in S\right) \\
&amp; = n\mathbb{P} \left( x_1 \in S\right) 
\end{align}
\]</span></p>
<p>What is <span class="math inline">\(\mathbb{P} \left( x_1 \in S\right)\)</span> ? Well, this is fairly simple if we instead calculate <span class="math inline">\(\mathbb{P} \left( x_1 \notin S\right)\)</span>:</p>
<p><span class="math display">\[
\begin{align}
\mathbb{P} \left( x_1 \notin S\right) 
&amp;= \prod_{i = 1}^{n}  \mathbb{P} \left(x^{*}_i \notin S\right) \\
&amp;= \left(1 - \frac{1}{n}\right)^n
\end{align}
\]</span></p>
<p>Note that <span class="math inline">\(x_i^*\)</span> represent the <span class="math inline">\(i^{th}\)</span> element in the <strong>bootstrap sample</strong> whereas <span class="math inline">\(x_i\)</span> represent the <span class="math inline">\(i^{th}\)</span> element in the <strong>original sample</strong>. Therefore, the expected number of unique data points in a bootstrap sample, as a <strong>proportion</strong> of total data points is:</p>
<p><span class="math display">\[
\begin{align}
\mathbb{E} \left[\frac{Y}{n}\right] 
&amp;= \mathbb{P} \left( x_1 \in S\right) \\
&amp;= 1 -  \mathbb{P} \left( x_1 \notin S\right) \\
&amp;= 1 - \left( 1 - \frac{1}{n} \right)^n
\end{align}
\]</span></p>
<p>So the expected proportion still depends on n, but it converges to <span class="math inline">\(1 - e^{-1}\)</span> very quickly as n gets large:</p>
<p><img src="/post/2018-01-21-statistical-learning-how-to-prove-a-bootstrap-sample-is-expected-to-contain-63-2-two-thirds-of-original-data-points_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>We can see that the expected proportion gets very close to 0.632 when sample size is greater than 20.</p>
<p>So the easy way is fairly easy. But it only gives you the mean of the distribution. What does the distribution of <em>the proportion of original data points in a bootstrap sample</em> look like?</p>
</div>
<div id="the-hard-way" class="section level2">
<h2>The hard way</h2>
<p>Another way to get to calculate an expected value is to derive the distribution first and then take the expection. It doesn’t sound hard, but it does require me to devote more brain energy. As humans are lazy, I will just come back to it later when I have time (blame human not me). But here’s a <a href="https://arxiv.org/pdf/1602.05822.pdf">paper</a> that solves exactly what I failed to solve.</p>
<p>The paper discusses the distribution of <em>the number of unique original data points in a bootstrap sample</em>, which is simply the distribution of <em>the proportion of unique orignial data points in a bootstrap sample</em> scaled by sample size. Standing on their shoulder, I know the probability of obtaining <span class="math inline">\(k\)</span> unique original data points is:</p>
<p><span class="math display">\[
\mathbb{P}(k) = \frac{(n)_k}{n^n} { n \brace k }
\]</span></p>
<p>where <span class="math inline">\(n\)</span> is sample size, <span class="math inline">\((n)_k = \frac{n!}{(n-k)!}\)</span> is a falling factorial, and <span class="math inline">\({n \brace k}\)</span> is a <a href="https://en.wikipedia.org/wiki/Stirling_numbers_of_the_second_kind">Stirling number of the second kind</a> (It basically represents number of ways to partition a set of n objects into k non-empty subsets.). If I denote <span class="math inline">\(p\)</span> as the <em>the proportion of unique orignial data points in a bootstrap sample</em> (<span class="math inline">\(p = \frac{k}{n}\)</span>), then:</p>
<p><span class="math display">\[
\begin{align}
\mathbb{P}_P(p) 
&amp;= \mathbb{P}_K(np) \\ 
&amp;= \frac{(n)_{np}}{n^n} { n \brace np } \\
\end{align}
\]</span></p>
<p>With the help of <code>gmp</code> package, I can visualize this distribution, although it took me some time to grasp <code>bigz</code> and <code>bigq</code> class implemented in <code>gmp</code> package. They are needed because <span class="math inline">\(n^n\)</span>, <span class="math inline">\((n)_{np}\)</span> and <span class="math inline">\({n \brace np}\)</span> involves incredibly large integers that can easily exceed the capability of R’s 32 bit integer. <span class="math inline">\(20^{20} \approx 10^{26}\)</span> which already exceeds maximum integer in R by quite a big margin.</p>
<p>Below is a graph showing the distribution for n = 10, 50 and 250. One can almost imaging that when n gets larger, say 1000, the distribution will basically concentrate on a very narrow range around 0.632. Note some points are wrong because I ran into precision problems and I guess it’s because R loses precision in floating point arithmatics as the numbers involved are too big. I guess I should be able to avoid this by using <code>gmp</code> more properly, but I’ll leave it to the next time.</p>
<p><img src="/post/2018-01-21-statistical-learning-how-to-prove-a-bootstrap-sample-is-expected-to-contain-63-2-two-thirds-of-original-data-points_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
</div>
