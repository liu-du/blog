---
title: 'Deep Learning in R 2: compose logistic regressions == neural network'
author: ''
date: '2018-03-05'
slug: deep-learning-in-r-2-compose-logistic-regressions-neural-network
categories:
  - Deep Learning
tags:
  - deeplearning.ai
  - R
draft: true
output:
  blogdown::html_page:
    toc: true
---

## Intro

So it turns out I rushed through the deeplearning.ai specialization in one month. While I did get a lot of satisfaction and fun during the month, when I look at the two facts that I made recently, I feel a bit ironic:

- I keep paying a gym (50 bucks per month) which I pretty much never go to and I doesn't bother cancel;
- I rushed through the 5 good deep learning courses in less than a month to avoid paying $64 for one extra month.

Perhaps it's time for me to reflecting on my personal financial management and reweight what things I should value. You wouldn't believe if I tell you that I have a finance major. (I can't believe I do!)

That's enough of self criticism. I __strongly__ recommend you to take this specialization. They're just __so good__, period. You'll __regret__ if you don't take it, period. You __should__ take it, period.

In the last post, I implemented logistic regression from scratch in R. In this post, let's see how we can compose multiple logistic regressions to get a neural network. 


## Review

Let me remind myself how I implemented logistic regression one month earlier:

First, define a helper function, `sigmoid`, which is another name for inverse logit function. It is one of the activation functions in deep learning:

```{r}
sigmoid <- function(x) {
  1 / (1 + exp(-x))
}
```

Next implement the algorithm to optimize weights (a.k.a coefficients) in the following steps:

1. initialize weights (coefficients) to zeros;

2. forward propagation, i.e., calculating the predictions and cost based on the current weights;

```{r}
forward_propagation <- function(X, w, activation = sigmoid) {
  activation(X %*% w)
}

compute_cost <- function(y, a, N = length(y)) {
  -(1 / N) * (sum(y * log(a)) + sum((1 - y) * log(1 - a)))
}
```

3. backward propagation, i.e. calculating the gradients; 

```{r}
backward_propagation <- function(X, y, a, N = length(y)) {
  (1/N) * crossprod(X, a - y)
}
```


4. update the weights by moving them down the steepest slope. (a.k.a __Gradient descent__); 

```{r}
update_grads <- function(w, dw, learn_rate) {
  w - learn_rate * dw
}
```

5. repeat the step 2 - 4 a lot of times. I removed the code for caching intermediate results so the _logic_ is more prominent.

```{r}
logistic_model <- function(X, y, activation = sigmoid, learn_rate, iter) {
  
  N <- nrow(X)
  
  # initialize weights, cost
  w <- numeric(ncol(X))
  J <- numeric(iter)
  
  # optimize
  for (i in 1:iter) {
    
    # forwardprop, compute cost, backprop, gradient descent 
    a_train <- forward_propagation(X, w, activation = activation) 
    J[i] <- compute_cost(y, a_train, N = N) 
    dw <- backward_propagation(X, y, a_train, N = N) 
    w <- update_grads(w, dw, learn_rate = learn_rate)
    
  }
  
  # return a object of class `simple_lr` so we can write methods
  structure(
    list(J = J, w = w, dw = dw),
    class = "simple_lr"  
  )
}
```


To test that this simple logistic regression model is implemented correctly, let's compare it to `glm`.
```{r}
# our logistic_model requires data in matrix form, let's do 100 iteration
simple_lr_result <- logistic_model(X = model.matrix(vs ~ 0 + disp + mpg + hp, mtcars), y = mtcars$vs, learn_rate = 0.00015, iter = 30000)
plot(simple_lr_result$J, type = "l", lwd = 2,
     ylab = "Cost", xlab = "Iteration", main = "Cost function")
simple_lr_result$w
```


```{r}
# more elegant glm call with binomial family
glm_result <- glm(vs ~ 0 + disp + mpg + hp, mtcars, family = binomial(link = "logit"))
glm_result$coefficients
```

Great they agree! Clearly `glm` is more elegant and it also turns out it has a much better optimazation algorithm. It turns out that `logistic_regression` doesn't do a good job in optimizing the cost function, particularly if we add the intercept term (a.k.a bias) in. I will come back to this problem in another post to investigate more optimization algorithms.

Here's a helper method for predicting with this simple logistic regression object.
```{r}
predict.simple_lr <- function(object, newdata, type = c("link", "response", "label")) {
  
  type = match.arg(type)
  
  if (type == "link") return(drop(newdata %*% object$w))
  
  # one more step of forward propagation
  response <- drop(forward_propagation(newdata, w = object$w))
  
  if (type == "label") ifelse(response > 0.5, 1, 0) else response
}
```

```{r}
predict(glm_result, newdata = mtcars, type = "response") 
```

```{r}
predict(simple_lr_result, newdata = model.matrix(vs ~ 0 + disp + mpg + hp, mtcars), type = "response")
```

To summarise, logistic regression takes as input some data, $X$, linearly combines them using it's weights, $Xw$, and applies a non-linear activation function (e.g. `sigmoid` function) to make predictions $\hat{y}$. Or more generally, it takes some __input__ $X$ and produces __output__ $\hat{y}$.

## Compose logistic regression

In the field of deep learning, logistic regression can be viewed as the basic _building block_ of a neural network, hence it is also called a _logistic unit_ or simply a _unit_. It is als called a __neuron__, inspired by neuron science^[To know more about the analogy between human brain and an artifical neural network model, check out this coursera [video](https://www.coursera.org/learn/neural-networks/lecture/V1GXW/what-are-neural-networks-8-min) by [Geoffrey Hinton](https://en.wikipedia.org/wiki/Geoffrey_Hinton), one of the most influencial poineers of the neural networks.]

Each __neuron__ (logistic regression) takes previous neurons' outputs (previous logistic regression units' __predictions__, or more commonly refered to as __activations__ in deep learning) as it's inputs, and computes it's outputs, which will be fed into it's downstream neurons. Note that each neuron takes many neurons's outputs as it's inputs, i.e., it depends on more than one neurons. 

Perhaps for cleaner organization, neurons are organized into __hidden layers__ and __output layers__. Hidden layers are comprised of __hidden units__ (i.e. logistic regressions) and output layers are comprised of one or more units, or neurons, that output final predictions. The input matrix, $\mathbf{X}$ is organized into an __input layer__, which is comprised or $p$ units, where $p$ is the number of features. For the $i{th}$ input unit, an N-vector, $\mathbf{x}_i$ represents all $N$ observations for that unit. 

Here I am following traditional statistics notations (adapted from [The Elements of Statisitcal Learning](https://web.stanford.edu/~hastie/Papers/ESLII.pdf)): all vectors are column vectors denoted by non-bold lower case letters (e.g. $w$), except when a vector has $N$ (sample size) components in which case it is denoted by __bold__ lower case letters ($\mathbf{a}$). Elements of a vector can be accessed via subscript (e.g. $w_j$ represent the $j^{th}$ element of $w$). Matrices are denoted by __bold__ capital letters (e.g. $\mathbf{X}$) and elements can be accessed by $\mathbf{X}_{ij}$. Non bold capital letters are used to denote the generic aspect of a random variable, i.e., a feature.

This notation, however, doesn't differentiate $\mathbf{x}_i$, which represents an N-vector of all observations for $i^{th}$ feature from $\mathbf{x}_i$, which represents the $i^{th}$ entry of an N-vector $\mathbf{x}$. This isn't super important now because most of the time we don't refer to individual entries in a vector. I will however make it clear when individual entries need to be refered to.

The graph below shows a simple such neural network, with an input layer comprising of $p$ features, a hidden layer with 4 hidden units ,i.e., 4 logistic regressions, and an output layer with one unit ,i.e., 1 logistic regression. Note that this graph shows __conceptually__ the structure (or architecture) of a neural network, because I am using the uppercase captical letters to stress the generic aspects of the variables, or features.

```{r include=FALSE}
library(igraph)
```

```{r echo=FALSE}
nodes <- data.frame(
  id = c("x1", "x2", "x99", "x100", "a11", "a12", "a13", "a14", "a21", "J")
)

links <- data.frame(
  from = c(rep(c("x1", "x2", "x99", "x100"), c(4, 4, 4, 4)), c("a11", "a12", "a13", "a14"), "a21"),
  to = c(rep(c("a11", "a12", "a13", "a14"), 4), rep("a21", 4), "J")
)

net <- graph_from_data_frame(d = links, vertices = nodes, directed = T)
plot(net, 
     layout = matrix(c(0, 0, 1, 0, 4, 0, 5, 0, 
                       1, 1, 2, 1, 3, 1, 4, 1,
                       2.5, 2, 2.5, 3), ncol = 2, byrow = TRUE), 
     edge.arrow.size = 0.3,
     vertex.size = c(rep(35, 9), 50),
     vertex.color = "#FFFFFF00",
     vertex.label.color = "black",
     vertex.label = expression(X[1], X[2], X[p - 1], X[p], 
                               A[1]^group("[", 1, "]"), A[2]^group("[", 1, "]"), A[3]^group("[", 1, "]"), A[4]^group("[", 1, "]"), 
                               A[1]^group("[", 2, "]"), J:Cost),
     main = quote(A ~ bold(Conceptual) ~ neural ~ network))

symbols(y = rep(-1, 3) , x = c(0.25, 0, -0.25), circles = rep(0.01, 3), inches = FALSE, bg = "black", add = TRUE)
```

In the graph above, superscript in square bracket represents the __layer number__ whereas subscript identifies logistic units. So $A_2^{[1]}$ means the second logistic regression unit, or the second hidden unit in layer 1. Note that $X_i$ denotes the $i^{th}$ variable, or feature.


Empirically, once we get labelled training data (size $N$), we can substitute the non bold uppercase capital letters, denoting generic aspects of variables, by __bold__ lower case letters that represent $N$ __realizations__ of those variables. That is, each __bold__ lower case letter denotes an N-vector that represents all observations for the corresponding variable in the conceptual graph above:

```{r echo=FALSE}
nodes <- data.frame(
  id = c("x1", "x2", "x99", "x100", "a11", "a12", "a13", "a14", "a21", "J")
)

links <- data.frame(
  from = c(rep(c("x1", "x2", "x99", "x100"), c(4, 4, 4, 4)), c("a11", "a12", "a13", "a14"), "a21"),
  to = c(rep(c("a11", "a12", "a13", "a14"), 4), rep("a21", 4), "J")
)

net <- graph_from_data_frame(d = links, vertices = nodes, directed = T)
plot(net, 
     layout = matrix(c(0, 0, 1, 0, 4, 0, 5, 0, 
                       1, 1, 2, 1, 3, 1, 4, 1,
                       2.5, 2, 2.5, 3), ncol = 2, byrow = TRUE), 
     edge.arrow.size = 0.3,
     vertex.size = c(rep(35, 9), 50),
     vertex.color = "#FFFFFF00",
     vertex.label.color = "black",
     vertex.label = expression(bold(x)[1], bold(x)[2], bold(x)[p - 1], bold(x)[p], 
                               bold(a)[1]^group("[", 1, "]"), bold(a)[2]^group("[", 1, "]"), bold(a)[3]^group("[", 1, "]"), bold(a)[4]^group("[", 1, "]"), 
                               bold(a)[1]^group("[", 2, "]"), J:Cost),
     main = quote(An ~ bold(Empirical) ~ neural ~ network))

symbols(y = rep(-1, 3) , x = c(0.25, 0, -0.25), circles = rep(0.01, 3), inches = FALSE, bg = "black", add = TRUE)
```

Note that $\mathbf{x}_i$ and $\mathbf{a}_i^{[j]}$ are all N-vectors, where $\mathbf{x}_i$ represents observations and $\mathbf{a}_i^{[j]}$ represents intermediate values the neural network computes in order to do final predictions. It feels natuaral or cognitively easier if we generalize the notation for hidden layers and output layers to input layers as well:

```{r echo=FALSE}
nodes <- data.frame(
  id = c("x1", "x2", "x99", "x100", "a11", "a12", "a13", "a14", "a21", "J")
)

links <- data.frame(
  from = c(rep(c("x1", "x2", "x99", "x100"), c(4, 4, 4, 4)), c("a11", "a12", "a13", "a14"), "a21"),
  to = c(rep(c("a11", "a12", "a13", "a14"), 4), rep("a21", 4), "J")
)

net <- graph_from_data_frame(d = links, vertices = nodes, directed = T)
plot(net, 
     layout = matrix(c(0, 0, 1, 0, 4, 0, 5, 0, 
                       1, 1, 2, 1, 3, 1, 4, 1,
                       2.5, 2, 2.5, 3), ncol = 2, byrow = TRUE), 
     edge.arrow.size = 0.3,
     vertex.size = c(rep(35, 9), 50),
     vertex.color = "#FFFFFF00",
     vertex.label = expression(bold(a)[1]^group("[", 0, "]"), bold(a)[2]^group("[", 0, "]"), bold(a)[p - 1]^group("[", 0, "]"), bold(a)[p]^group("[", 0, "]"), 
                               bold(a)[1]^group("[", 1, "]"), bold(a)[2]^group("[", 1, "]"), bold(a)[3]^group("[", 1, "]"), A[4]^group("[", 1, "]"), 
                               bold(a)[1]^group("[", 2, "]"), J:Cost),
     vertex.label.color = c(rep("darkorange2", 4), rep("black", 6)),
     main = quote(An ~ bold(Empirical) ~ neural ~ network ~ alternative ~ notation))

symbols(y = rep(-1, 3) , x = c(0.25, 0, -0.25), circles = rep(0.01, 3), inches = FALSE, bg = "black", add = TRUE)
```

where $\mathbf{a}_i^{[0]} = \mathbf{x}_i \forall i \in \{1, 2, .., p\}$.

Note that the input layer, by convention, is layer 0, and doesn't count towards total number of layers. So the network above is said to have two fully connected layers, where __a fully connected layer__ means that each neuron in that layer takes outputs from all neurons in the previous layer as its inputs. 

The design matrix can also be denoted using $\mathbf{A}^{[0]}$ and can be written as:

$$
\begin{align}
\mathbf{X}
& = \mathbf{A^{[0]}} \\
& = \begin{bmatrix} \mathbf{1} \ \mathbf{x}_1 \ \mathbf{x}_2 \ ...\ \mathbf{x}_p \end{bmatrix} \\
& = \begin{bmatrix} \mathbf{1} \ \mathbf{a}_1^{[0]} \ \mathbf{a}_2^{[0]} \ ...\ \mathbf{a}_p^{[0]} \end{bmatrix} \\
\end{align}
$$


### Forward Propagation
The forward propagation for a single logistic regression is quite simple:

$$
\mathbf{a} = \sigma(\mathbf{X}w)
$$
Where $\mathbf{X}$ is the $N$ by $p$ design matrix, $w$ is the (p + 1)-vector representing coefficients, or biases and weights. So for the $i^{th}$ neuron in layer $1$ (the middle layer), we can write:

$$
\mathbf{a}_i^{[1]} = \sigma(\mathbf{X}w_i^{[1]}) = \sigma(\mathbf{A}^{[0]}w_i^{[1]}) 
$$

More generally, for the $i^{th}$ neuron in layer l:

$$
\mathbf{a}_i^{[l]} = \sigma(\mathbf{z}_i^{[l-1]}) = \sigma(\mathbf{A}^{[l - 1]} w_i^{[l]}) 
$$

Again, we need to write down equations in more compact form the take advantage of vectorization, so if we stack all $A_i^{[l]}$ in layer l, (i.e. stack all activations from each logistic unit) horizontally and prepend a vector of 1 to take care of biases, we have:

$$
\begin{align}
\mathbf{A}^{[l]} 
& = \begin{bmatrix} \mathbf{1} \ \mathbf{a}_1^{[l]} \ \mathbf{a}_1^{[l]} \ ... \ \mathbf{a}_p^{[l]}  \end{bmatrix}  \\
& = \begin{bmatrix} \mathbf{1} \ \sigma(\mathbf{z}_1^{[l - 1]}) \ \sigma(\mathbf{z}_2^{[l - 1]}) \ ... \ \sigma(\mathbf{z}_p^{[l - 1]}) \end{bmatrix} \\
& = \begin{bmatrix} \mathbf{1} \ \sigma(\mathbf{A}^{[l - 1]} w_1^{[l]}) \ \sigma(\mathbf{A}^{[l - 1]} w_2^{[l]}) \ ... \ \sigma(\mathbf{A}^{[l - 1]} w_p^{[l]}) \end{bmatrix} \\
& = \begin{bmatrix} \mathbf{1} \ \sigma(\mathbf{Z}^{[l - 1]}) \end{bmatrix} \\
& = \begin{bmatrix} \mathbf{1} \ \sigma(\mathbf{A}^{[l - 1]}W^{[l]}) \end{bmatrix} \\
\end{align}
$$

The above equation is a recursive one, which means we can get $A^{[l]}$ if we know $A^{[0]}$ (which we know is simple $X$) and all the weights. So I won't bother writing down the exact equation of $A^{[l]}$ in terms of $X$ and $W^{[l]}$. The cost function, perhaps not surprisingly, only depends on the final prediction, i.e., the activations of the final layer (which comprises only one unit):

$$
J(W^{[1]}, W^{[1]}, ..., W^{[L]})  = - \frac{1}{N} \left( \mathbf{y}^T log(A^{[L]}) + (\mathbf{1} - \mathbf{y})^T log(\mathbf{1} - A^{[L]}) \right)
$$

Where $L$ is the number of layers (2 in this example). Note that $A^{[L]}$ reduces to an $N$ by $1$ matrix, or equivalently an N-vector.

### Backward Propagation

Here comes the (perhaps) hard part. In order for the algorithm to learn, for each iteration of forward and backward pass, we need to adjust every weight for every layer, that is, we need to compute derivatives of cost with respect to each weight so we can do gradient desecent. Again, let's solve this recursively: compute the derivative of cost, J, with respect to the activations and weights in the final layer, $A^{[L]}$, and then back propagate to previous layers, hence previous weights.

Similar to the last post, let's divide and conquer. The cost $J$ can be break down into the average of loss functions:

$$
\begin{align}
J 
& = - \frac{1}{N} \left( \mathbf{y}^T log(A^{[L]}) + (\mathbf{1} - \mathbf{y})^T log(\mathbf{1} - A^{[L]}) \right) \\
& = \frac{1}{N} \left(- \mathbf{y}^T log(A^{[L]}) + (\mathbf{1} - \mathbf{y})^T log(\mathbf{1} - A^{[L]}) \right) \\
& = \frac{1}{N} \sum_i^N \left( - \mathbf{y_i} log(A_i^{[L]}) + (\mathbf{1} - \mathbf{y_i}) log(\mathbf{1} - A_i^{[L]}) \right) \\
& = \frac{1}{N} \sum_i^N C_i
\end{align}
$$

where $C_i = - \mathbf{y_i} log(A_i^{[L]}) + (\mathbf{1} - \mathbf{y_i}) log(\mathbf{1} - A_i^{[L]})$

Note that $\frac{dC_i}{dA_i} = \left( -\frac{\mathbf{y_i}}{\mathbf{a_i}} + \frac{1 - \mathbf{y_i}}{1 - \mathbf{a_i}} \right)$ 

For the $i^{[th]}$ weight in the final layer L: 

$$
\begin{align}
\frac{dJ}{dA^{[L]}} = 
& = \begin{bmatrix} \frac{dJ}{\mathbf{1}} \ \frac{dJ}{A_1^{[l]}} \ \frac{dJ}{ A_2^{[l]}} \ ... \ \frac{dJ}{A_p^{[l]}}  \end{bmatrix}  \\
& = \begin{bmatrix} \mathbf{0} \ \frac{dC_1}{A_1^{[l]}} \ \frac{dC}{ A_2^{[l]}} \ ... \ \frac{dC}{A_p^{[l]}}  \end{bmatrix}  \\
\end{align}
$$

$$

$$

### Iterate

### Implementation

## Summary

`Composing logistic regression == neural network` probably evaluates to `TRUE` in R. More importantly, something big behind seems to resonate with me strongly: start from small, well designed buiding block, by composing them together, you get a complex and __capable__ system:

> Make each program do one thing well. 
> --- Unix philosophy #1

By composing small programs together, you get unix operating systems.




