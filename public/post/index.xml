<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on R | Data science | Visualisation</title>
    <link>/post/</link>
    <description>Recent content in Posts on R | Data science | Visualisation</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 25 Sep 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Encode boolean values and operations with pure functions</title>
      <link>/2018/09/25/encode-boolean-and-logical-operations-with-pure-functions/</link>
      <pubDate>Tue, 25 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/09/25/encode-boolean-and-logical-operations-with-pure-functions/</guid>
      <description>Computerphile has a really good video that covers the fundamental ideas of lambda calculus, which is the basis of most functional programming languages. One of the example it went throught is encoding boolean values (true/false) using pure functions - it really bended my mind.
The fundamental idea of a boolean value is making a choice. So, the encoding of boolean values encode that idea! The true value can be encoded as a function that takes two inputs, x and y, and return the first input, x, that is, it chooses the first input.</description>
    </item>
    
    <item>
      <title>Datasets in R</title>
      <link>/2018/06/09/datasets-in-r/</link>
      <pubDate>Sat, 09 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/06/09/datasets-in-r/</guid>
      <description>Quite often, I find myself creating dataframes to test if my functions work on different types of columns, or if their results are reasonable. Creating these “test cases” isn’t the most fun thing to do, it’s take up a lot of time and because I often just throw them away once I’m done, I have to recreate them next time. So, can I leverage a package to do this for us?</description>
    </item>
    
    <item>
      <title>R: from on.exit() to quasiquotation</title>
      <link>/2018/03/26/r-on-exit-and-sys-on-exit/</link>
      <pubDate>Mon, 26 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/03/26/r-on-exit-and-sys-on-exit/</guid>
      <description>Part 1Often one needs to write functions that not only perform computation on its own, but also interact with the world outside of the function execution environment, for example, writing to a file, saving plots or changing working directory. Computer scientists called these operations side effects, in the sense that these functions change some aspects of the global state of the software. To be more precise, writing to a file requires a file connection to be established, saving plots requires opening a graphical device and changing working directories affects how to find things in the computer file system.</description>
    </item>
    
    <item>
      <title>Deep Learning in R 2: compose logistic regressions == neural network</title>
      <link>/2018/03/05/deep-learning-in-r-2-compose-logistic-regressions-neural-network/</link>
      <pubDate>Mon, 05 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/03/05/deep-learning-in-r-2-compose-logistic-regressions-neural-network/</guid>
      <description>IntroReviewCompose logistic regressionForward PropagationBackward PropagationIterateImplementationSummaryIntroSo it turns out I rushed through the deeplearning.ai specialization in one month. While I did get a lot of satisfaction and fun during the month, when I look at the two facts that I made recently, I feel a bit ironic:
I keep paying a gym (50 bucks per month) which I pretty much never go to and I doesn’t bother cancel;I rushed through the 5 good deep learning courses in less than a month to avoid paying $64 for one extra month.</description>
    </item>
    
    <item>
      <title>Deep Learning in R 1: logistic regression with a neural netwrok mindset from scratch in R</title>
      <link>/2018/02/10/deep-learning-neural-network-from-scratch-in-r/</link>
      <pubDate>Sat, 10 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/02/10/deep-learning-neural-network-from-scratch-in-r/</guid>
      <description>IntroData PreparationLogistic RegressionForward PropagationBackward PropagationImplementationTrain ModelUnderstand the ResultSummaryIntroWith online courses, I feel I am usually too rushed to complete them for at least two reasons:
Save money because I pay a monthly fee.Eager to get a certificate so I can overestimate myself.So this time I think perhaps I should pause and let it precipitate.</description>
    </item>
    
    <item>
      <title>Statistical Learning: Cross Validation wrong and right way</title>
      <link>/2018/01/27/statistical-learning-cross-validation/</link>
      <pubDate>Sat, 27 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/01/27/statistical-learning-cross-validation/</guid>
      <description>IntroThe Wrong WayThe Right WaysummaryIntroCross validation is one of the most popular methods for estimating test error and selecting tuning parameters, however, one can easily be mislead/confused by it without realising it.
In chapter 5 of An Introduction to Statistical learning, the authors stress that if a dataset has a lot of features and relatively fewer observations, the variable selection process should also be cross validated.</description>
    </item>
    
    <item>
      <title>Statistical Learning: a bootstrap sample contains two thirds of original data points?</title>
      <link>/2018/01/21/statistical-learning-how-to-prove-a-bootstrap-sample-is-expected-to-contain-63-2-two-thirds-of-original-data-points/</link>
      <pubDate>Sun, 21 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/01/21/statistical-learning-how-to-prove-a-bootstrap-sample-is-expected-to-contain-63-2-two-thirds-of-original-data-points/</guid>
      <description>IntroRecently, I am studying The Elements of Statistical Learning out of my own interest. An interesting topic they covered briefly on their online videos is bootstrap. Bootstrap, in the simplest terms, is the process of generating samples from sample. Say you collected a sample of n data points, a bootstrap sample is generated by sampling n data points from the original sample you collected with replacement. (So Given a sample size of n, you are able to generate \(n^n\) bootstrap samples.</description>
    </item>
    
    <item>
      <title>R: Setting options</title>
      <link>/2018/01/07/setting-options-in-r/</link>
      <pubDate>Sun, 07 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/01/07/setting-options-in-r/</guid>
      <description>When developing a package or a set of functions, you often needs a lot of options. Often, you would want to set some sensible defaults for each option whilst giving users the flexibility to customize and extend. Take ggplot2 for example, if you ever used it, you know the background of the plots is, by default, grey, though You have the flexibility to change it.
While there are many ways to set options, I haven’t found a summary of pros and cons of different approaches and I certainly don’t know what best practices are.</description>
    </item>
    
    <item>
      <title>My first blog post</title>
      <link>/2018/01/01/my-first-blog-post/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/01/01/my-first-blog-post/</guid>
      <description>If you’re not prepared to be wrong, you’ll never come up with anything original.
— Ken Robinson in a Ted Talk
Starting this blog as new year resolution!
While building this blogdown website is made so easy with blogdown package, I did have a difficult time setting up ssh for use. Perhaps I should learn the ssh and do it as my second post? I also want to talk about R, vim, building websites as I learn it.</description>
    </item>
    
  </channel>
</rss>