<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Data and Code</title>
    <link>/post/</link>
    <description>Recent content in Posts on Data and Code</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 29 Sep 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Learn Haskell 4: class</title>
      <link>/2019/09/29/learn-haskell-4-class/</link>
      <pubDate>Sun, 29 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/09/29/learn-haskell-4-class/</guid>
      <description>We can see this in other languages as well, for example, in scala’s type hierachy, Tuples inherits from Product.
Either is the canonical Sum type (a.k.a Coproduct).
data Either a b = Left a | Right b Part 2: Class This is similar to inheritance in OOP. Similar to java interface
data Tree a = Empty | Node (Tree a) a (Tree a) deriving Show :{ class MyFunctor f where fmap :: (a -&amp;gt; b) -&amp;gt; f a -&amp;gt; f b instance MyFunctor Tree where fmap _ Empty = Empty fmap f (Node l m r) = Node (fmap f l) (f m) (fmap f r) :} tree = Node (Node Empty 1 Empty) 2 Empty fmap (* 2) tree  ## Node (Node Empty 2 Empty) 4 Empty functions are containers, e.</description>
    </item>
    
    <item>
      <title>Learn Haskell 3: Algebraic Data Types</title>
      <link>/2019/09/28/learn-haskell-3/</link>
      <pubDate>Sat, 28 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/09/28/learn-haskell-3/</guid>
      <description>A recent revelation I had is: all I’m doing when I program is just manipulating Algebraic Data Types.
What are algebraic data types? they are simply data types that you can do algebra with them. The most basic algebra, aka high school algebra, are addition and multiplication. Instead of doing algebra on numbers, we can also do algebra on types and the resulting types are called algebraic data types. The two fundamental algebraic data types are Sum type which corresponds to “adding” two types, and Product type which corresponds to “multiplying” two types.</description>
    </item>
    
    <item>
      <title>Learn Haskell 1: Hello world</title>
      <link>/2019/09/08/learn-haskell-1/</link>
      <pubDate>Sun, 08 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/09/08/learn-haskell-1/</guid>
      <description>--- title: &#34;Learn Haskell 1: Hello world&#34; author: &#39;&#39; date: &#39;2019-09-08&#39; slug: learn-haskell-1 categories: - functional programming - programming tags: - Functional Programming --- Learn Haskell by going through Bartosz Milewski’s youtube lectures.
Lecture 1 In Haskell, you don’t need brackets for function application, so here’s goes the classic hello-world example:
putStrLn &amp;quot;Hello, world!&amp;quot; ## Hello, world! This doesn’t look any different to other programming languages, but the mind blowing thing is, putStrLn is actually not causing any side effect.</description>
    </item>
    
    <item>
      <title>Learn Haskell 2: Function and Data Type</title>
      <link>/2019/09/08/learn-haskell-2/</link>
      <pubDate>Sun, 08 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/09/08/learn-haskell-2/</guid>
      <description>Part 1: function, currying, pattern matching and function composition It makes sense that if some pattern/idea is used a lot, then it should be denoted with minial syntax. It follows that Haskell, famous for being functional, doesn’t even have a keyword for defining functions. To define a function, start with function name (add for example), followed by argument list separated by spaces, followed by = and then the function body:</description>
    </item>
    
    <item>
      <title>Implement Random Forest</title>
      <link>/2018/10/15/implement-random-forest/</link>
      <pubDate>Mon, 15 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/10/15/implement-random-forest/</guid>
      <description>I’ve been playing with many different machine/statistical learning models for quite some time and I think I’m now at a point where if I want to keep improving my skills/understanding to the next level, I really need to implement these algorithms from scratch myself.
Random Forest is a classical machine learning algorithm and I have a feeling that it should be relatively easy to implement - it is essentially just a collection of tree models.</description>
    </item>
    
    <item>
      <title>A Lisp interpreter in R, part 1</title>
      <link>/2018/10/12/write-a-lisp-interpreter-in-r-part-1/</link>
      <pubDate>Fri, 12 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/10/12/write-a-lisp-interpreter-in-r-part-1/</guid>
      <description>How do computer languages work? How can they understand programs and perform computation?
 So, let’s implement the famous computer language, Lisp, in R. Or more specifically, let’s write an interpreter for lisp language in R.
Lisp stands for List Processing. As it’s name suggests, the programs written in lisp are actually lists and Lisp processes the programs (lists) and give you back an answer.
An interpreter is largely comprised of two parts:</description>
    </item>
    
    <item>
      <title>Encode boolean values and operations with pure functions</title>
      <link>/2018/09/25/encode-boolean-and-logical-operations-with-pure-functions/</link>
      <pubDate>Tue, 25 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/09/25/encode-boolean-and-logical-operations-with-pure-functions/</guid>
      <description>Computerphile has a really good video that covers the fundamental ideas of lambda calculus, which is the basis of most functional programming languages. One of the example it went throught is encoding boolean values (true/false) using pure functions - it really bended my mind.
The fundamental idea of a boolean value is making a choice. So, the encoding of boolean values encode that idea! The true value can be encoded as a function that takes two inputs, x and y, and return the first input, x, that is, it chooses the first input.</description>
    </item>
    
    <item>
      <title>Datasets in R</title>
      <link>/2018/06/09/datasets-in-r/</link>
      <pubDate>Sat, 09 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/06/09/datasets-in-r/</guid>
      <description>Quite often, I find myself creating dataframes to test if my functions work on different types of columns, or if their results are reasonable. Creating these “test cases” isn’t the most fun thing to do, it’s take up a lot of time and because I often just throw them away once I’m done, I have to recreate them next time. So, can I leverage a package to do this for us?</description>
    </item>
    
    <item>
      <title>R: from on.exit() to quasiquotation</title>
      <link>/2018/03/26/r-on-exit-and-sys-on-exit/</link>
      <pubDate>Mon, 26 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/03/26/r-on-exit-and-sys-on-exit/</guid>
      <description>Part 1Often one needs to write functions that not only perform computation on its own, but also interact with the world outside of the function execution environment, for example, writing to a file, saving plots or changing working directory. Computer scientists called these operations side effects, in the sense that these functions change some aspects of the global state of the software. To be more precise, writing to a file requires a file connection to be established, saving plots requires opening a graphical device and changing working directories affects how to find things in the computer file system.</description>
    </item>
    
    <item>
      <title>Deep Learning in R 2: compose logistic regressions == neural network</title>
      <link>/2018/03/05/deep-learning-in-r-2-compose-logistic-regressions-neural-network/</link>
      <pubDate>Mon, 05 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/03/05/deep-learning-in-r-2-compose-logistic-regressions-neural-network/</guid>
      <description>IntroReviewCompose logistic regressionForward PropagationBackward PropagationIterateImplementationSummaryIntroSo it turns out I rushed through the deeplearning.ai specialization in one month. While I did get a lot of satisfaction and fun during the month, when I look at the two facts that I made recently, I feel a bit ironic:
I keep paying a gym (50 bucks per month) which I pretty much never go to and I doesn’t bother cancel;I rushed through the 5 good deep learning courses in less than a month to avoid paying $64 for one extra month.</description>
    </item>
    
    <item>
      <title>Deep Learning in R 1: logistic regression with a neural netwrok mindset from scratch in R</title>
      <link>/2018/02/10/deep-learning-neural-network-from-scratch-in-r/</link>
      <pubDate>Sat, 10 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/02/10/deep-learning-neural-network-from-scratch-in-r/</guid>
      <description>IntroData PreparationLogistic RegressionForward PropagationBackward PropagationImplementationTrain ModelUnderstand the ResultSummaryIntroWith online courses, I feel I am usually too rushed to complete them for at least two reasons:
Save money because I pay a monthly fee.Eager to get a certificate so I can overestimate myself.So this time I think perhaps I should pause and let it precipitate.</description>
    </item>
    
    <item>
      <title>Statistical Learning: Cross Validation wrong and right way</title>
      <link>/2018/01/27/statistical-learning-cross-validation/</link>
      <pubDate>Sat, 27 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/01/27/statistical-learning-cross-validation/</guid>
      <description>IntroThe Wrong WayThe Right WaysummaryIntroCross validation is one of the most popular methods for estimating test error and selecting tuning parameters, however, one can easily be mislead/confused by it without realising it.
In chapter 5 of An Introduction to Statistical learning, the authors stress that if a dataset has a lot of features and relatively fewer observations, the variable selection process should also be cross validated.</description>
    </item>
    
    <item>
      <title>Statistical Learning: a bootstrap sample contains two thirds of original data points?</title>
      <link>/2018/01/21/statistical-learning-how-to-prove-a-bootstrap-sample-is-expected-to-contain-63-2-two-thirds-of-original-data-points/</link>
      <pubDate>Sun, 21 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/01/21/statistical-learning-how-to-prove-a-bootstrap-sample-is-expected-to-contain-63-2-two-thirds-of-original-data-points/</guid>
      <description>IntroRecently, I am studying The Elements of Statistical Learning out of my own interest. An interesting topic they covered briefly on their online videos is bootstrap. Bootstrap, in the simplest terms, is the process of generating samples from sample. Say you collected a sample of n data points, a bootstrap sample is generated by sampling n data points from the original sample you collected with replacement. (So Given a sample size of n, you are able to generate \(n^n\) bootstrap samples.</description>
    </item>
    
    <item>
      <title>R: Setting options</title>
      <link>/2018/01/07/setting-options-in-r/</link>
      <pubDate>Sun, 07 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/01/07/setting-options-in-r/</guid>
      <description>When developing a package or a set of functions, you often needs a lot of options. Often, you would want to set some sensible defaults for each option whilst giving users the flexibility to customize and extend. Take ggplot2 for example, if you ever used it, you know the background of the plots is, by default, grey, though You have the flexibility to change it.
While there are many ways to set options, I haven’t found a summary of pros and cons of different approaches and I certainly don’t know what best practices are.</description>
    </item>
    
    <item>
      <title>My first blog post</title>
      <link>/2018/01/01/my-first-blog-post/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/01/01/my-first-blog-post/</guid>
      <description>If you’re not prepared to be wrong, you’ll never come up with anything original.
— Ken Robinson in a Ted Talk
Starting this blog as new year resolution!
While building this blogdown website is made so easy with blogdown package, I did have a difficult time setting up ssh for use. Perhaps I should learn the ssh and do it as my second post? I also want to talk about R, vim, building websites as I learn it.</description>
    </item>
    
  </channel>
</rss>